\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[brazil]{babel}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{geometry}
\geometry{margin=2.5cm}
\usepackage{hyperref}
\hypersetup{colorlinks=true,linkcolor=blue,urlcolor=blue}
\usepackage{enumerate}
\usepackage{array}

% Definições de ambientes
\theoremstyle{plain}
\newtheorem{teorema}{Teorema}[section]
\newtheorem{lema}[teorema]{Lema}
\theoremstyle{definition}
\newtheorem{definicao}[teorema]{Definição}
\theoremstyle{remark}
\newtheorem{observacao}[teorema]{Observação}

\title{Demonstrações dos Teoremas - Unidade 2\\
\large Convergência Estocástica e Resultados Limite\\
\normalsize Todas as Provas Apresentadas em Aula}
\author{Curso de Inferência Estatística}
\date{Outubro 2025}

\begin{document}

\maketitle
\tableofcontents
\newpage

\section{Introdução}

Este documento contém todas as demonstrações de teoremas apresentadas nas aulas da Unidade 2. O objetivo é fornecer um material de estudo organizado para preparação para as avaliações, onde demonstrações são frequentemente cobradas.

Ao final do documento, apresentamos um \textbf{ranking de prioridade} das demonstrações mais importantes para estudo, considerando complexidade técnica, importância fundamental e aplicabilidade em questões.

\section{Resultado 1P: Lei Fraca dos Grandes Números (Versão Simples)}

\begin{teorema}[LFGN - Versão Simples]
Sejam $X_1, \ldots, X_n$ v.a.'s i.i.d. com $\mathbb{E}(X_i) = \mu < \infty$ e $\mathrm{Var}(X_i) = \sigma^2 < \infty$. Então:
\[
\overline{X}_n \xrightarrow{P}_{n \to \infty} \mu
\]
\end{teorema}

\begin{proof}
Para um $\varepsilon > 0$ qualquer, pela desigualdade de Chebyshev:
\begin{align}
P\left( \left| \overline{X}_n - \mu \right| \geq \varepsilon \right) &= P\left( \left( \overline{X}_n - \mu \right)^2 \geq \varepsilon^2 \right) \\
&\leq \varepsilon^{-2} \, \mathbb{E} \left[ \left( \overline{X}_n - \mu \right)^2 \right] \\
&= \varepsilon^{-2} \, \mathrm{Var}(\overline{X}_n) \\
&= \varepsilon^{-2} \, \mathrm{Var}\left(\frac{1}{n}\sum_{i=1}^n X_i\right) \\
&= \varepsilon^{-2} \cdot \frac{1}{n^2} \sum_{i=1}^n \mathrm{Var}(X_i) \quad \text{(independência)} \\
&= \varepsilon^{-2} \cdot \frac{n\sigma^2}{n^2} \\
&= \frac{\sigma^2}{n \, \varepsilon^2} \xrightarrow{n \to \infty} 0
\end{align}

Logo, $\overline{X}_n \xrightarrow{P}_{n \to \infty} \mu$.
\end{proof}

\begin{observacao}[Pontos-chave da demonstração]
\begin{enumerate}
    \item Uso da desigualdade de Chebyshev para limitar a probabilidade
    \item Cálculo da variância da média amostral: $\mathrm{Var}(\bar{X}_n) = \sigma^2/n$
    \item A taxa de convergência é $O(1/n)$
\end{enumerate}
\end{observacao}

\section{Resultado 2P: Convergência via Momentos}

\begin{teorema}[Convergência via Momentos]
Sejam $\{T_n, n \geq 1\}$ uma sequência de variáveis aleatórias tais que para algum $r \geq 0$ e $a \in \mathbb{R}$ vale:
\[
\mathbb{E}\left[|T_n - a|^r\right] \xrightarrow{n \to \infty} 0
\]
Então $T_n \xrightarrow{P}_{n \to \infty} a$.
\end{teorema}

\begin{proof}
Para qualquer $\varepsilon > 0$, pela desigualdade de Markov:
\begin{align}
P\{|T_n - a| \geq \varepsilon\} &= P\{|T_n - a|^r \geq \varepsilon^r\} \\
&\leq \frac{\mathbb{E}[|T_n - a|^r]}{\varepsilon^r}
\end{align}

Como por hipótese $\mathbb{E}[|T_n - a|^r] \xrightarrow{n \to \infty} 0$, temos:
\[
P\{|T_n - a| \geq \varepsilon\} \leq \frac{\mathbb{E}[|T_n - a|^r]}{\varepsilon^r} \xrightarrow{n \to \infty} 0
\]

Portanto, $T_n \xrightarrow{P}_{n \to \infty} a$.
\end{proof}

\begin{observacao}[Utilidade]
Este resultado é muito útil porque:
\begin{enumerate}
    \item Basta verificar convergência de momentos (mais fácil de calcular)
    \item Funciona para qualquer $r > 0$, incluindo $r = 2$ (convergência em média quadrática)
    \item É frequentemente usado para provar que $S_n^2 \xrightarrow{P} \sigma^2$
\end{enumerate}
\end{observacao}

\section{Resultado 3P: Lei Fraca dos Grandes Números de Khinchin}

\begin{teorema}[LFGN de Khinchin]
Sejam $X_1, \ldots, X_n$ v.a.'s reais i.i.d. com $E[X_i] = \mu < \infty$. Então:
\[
\overline{X}_n \xrightarrow{P}_{n \to \infty} \mu
\]
\end{teorema}

\begin{proof}
Sejam $M_{\overline{X}_n}(t)$ e $M_{X_i}(t)$ as f.m.g. de $\overline{X}_n = \frac{1}{n} \sum_{i=1}^n X_i$ e $X_i$, respectivamente.

Para $t \in \mathbb{R}$:
\begin{align}
M_{\overline{X}_n}(t) &= M_{\frac{S_n}{n}}(t) = \mathbb{E}\left[e^{t\frac{S_n}{n}}\right] \\
&= \mathbb{E}\left[e^{\frac{t}{n}\sum_{i=1}^n X_i}\right] \\
&= \prod_{i=1}^n M_{X_i}\left(\frac{t}{n}\right) \quad \text{(independência)} \\
&= \left[ M_{X_1}\left(\frac{t}{n}\right) \right]^n \quad \text{(identicamente distribuídas)}
\end{align}

Expandindo $M_{X_1}\left(\frac{t}{n}\right)$ em série de Taylor em torno de zero até a 1ª ordem:

Dado que $M_{X_1}(0) = 1$ e $M'_{X_1}(0) = \mu$, temos:
\begin{align}
M_{X_1}\left(\frac{t}{n}\right) &= M_{X_1}(0) + M'_{X_1}(0) \frac{t}{n} + o\left(\frac{t}{n}\right) \\
&= 1 + \mu \frac{t}{n} + o\left(\frac{t}{n}\right)
\end{align}

Portanto:
\[
M_{\overline{X}_n}(t) = \left[ 1 + \frac{\mu t}{n} + o\left(\frac{t}{n}\right) \right]^n
\]

Usando o resultado limite $(R.3)$: $\left(1 + \frac{k}{n}\right)^n \xrightarrow{n \to \infty} e^k$:
\[
M_{\overline{X}_n}(t) \xrightarrow{n \to \infty} e^{t\mu} = M_{\mu}(t)
\]

Como a variável limite é degenerada em $\mu$, temos $\overline{X}_n \xrightarrow{P}_{n \to \infty} \mu$.
\end{proof}

\begin{observacao}[Vantagem sobre Resultado 1P]
Esta versão não requer variância finita, apenas média finita. É mais geral e poderosa!
\end{observacao}

\section{Resultado 5P: Teorema da Função Contínua}

\begin{teorema}[Teorema da Função Contínua para Convergência em Probabilidade]
Sejam $\{U_n, n \geq 1\}$ uma sequência de v.a.'s tal que $U_n \xrightarrow{P} u$ e $g(\cdot)$ uma função contínua. Então:
\[
g(U_n) \xrightarrow{P}{n \to \infty} g(u)
\]
\end{teorema}

\begin{proof}
Note que se $g(x)$ é contínua em $x = u$, então pela definição de continuidade: dado algum $\varepsilon > 0$, existe $\delta > 0$ tal que
\[
|x - u| < \delta \quad \Rightarrow \quad |g(x) - g(u)| < \varepsilon
\]

Equivalentemente, pela contrapositiva:
\[
|g(x) - g(u)| \geq \varepsilon \quad \Rightarrow \quad |x - u| \geq \delta
\]

Portanto, para $n$ suficientemente grande:
\begin{align}
0 &\leq P\left( |g(U_n) - g(u)| \geq \varepsilon \right) \\
&\leq P\left( |U_n - u| \geq \delta \right)
\end{align}

Como $U_n \xrightarrow{P} u$, sabemos que $P\left( |U_n - u| \geq \delta \right) \xrightarrow{n \to \infty} 0$.

Pelo teorema do confronto (squeeze theorem):
\[
P\left( |g(U_n) - g(u)| \geq \varepsilon \right) \xrightarrow{n \to \infty} 0
\]

Portanto, $g(U_n) \xrightarrow{P}{n \to \infty} g(u)$. \qed
\end{proof}

\begin{observacao}[Aplicação Importante]
Este teorema permite:
\begin{enumerate}
    \item Se $\bar{X}_n \xrightarrow{P} \mu$, então $(\bar{X}_n)^2 \xrightarrow{P} \mu^2$
    \item Se $S_n^2 \xrightarrow{P} \sigma^2$, então $S_n \xrightarrow{P} \sigma$
    \item Transformações de estimadores consistentes são consistentes
\end{enumerate}
\end{observacao}

\section{Teorema de Slutsky (Resultado 3D/39)}

\begin{teorema}[Teorema de Slutsky]
Sejam $\{U_n, n \geq 1\}$ e $\{V_n, n \geq 1\}$ duas sequências de v.a.'s tais que
\[
U_n \xrightarrow{d} U \quad \text{e} \quad V_n \xrightarrow{p} v \text{ (constante)}
\]
Então:
\begin{enumerate}
    \item $U_n + V_n \xrightarrow{d} U + v$
    \item $U_n V_n \xrightarrow{d} U \cdot v$
    \item $U_n / V_n \xrightarrow{d} U / v$, assumindo que $P(V_n = 0) = 0$, $\forall n$ e $v \neq 0$
\end{enumerate}
\end{teorema}

\begin{proof}[Esboço da prova do item (ii)]
Vamos provar que $U_n V_n \xrightarrow{d} U \cdot v$.

Podemos escrever:
\[
U_n V_n = U_n v + U_n(V_n - v)
\]

Precisamos mostrar que:
\begin{enumerate}
    \item $U_n v \xrightarrow{d} U v$ (multiplicação por constante preserva convergência em distribuição)
    \item $U_n(V_n - v) \xrightarrow{P} 0$
\end{enumerate}

Para o item (2): Como $U_n \xrightarrow{d} U$, a sequência $\{U_n\}$ é limitada em probabilidade, isto é, para qualquer $\eta > 0$, existe $M > 0$ tal que $P(|U_n| > M) < \eta$ para $n$ suficientemente grande.

Como $V_n \xrightarrow{p} v$, para qualquer $\delta > 0$, temos $P(|V_n - v| > \delta) \to 0$.

Portanto:
\begin{align}
P(|U_n(V_n - v)| > \varepsilon) &\leq P(|U_n| > M) + P(|U_n| \leq M, |V_n - v| > \varepsilon/M) \\
&< \eta + P(|V_n - v| > \varepsilon/M) \\
&\to \eta \quad \text{quando } n \to \infty
\end{align}

Como $\eta$ é arbitrário, $U_n(V_n - v) \xrightarrow{P} 0$.

Pelo teorema de Slutsky para soma, $U_n V_n = U_n v + U_n(V_n - v) \xrightarrow{d} Uv + 0 = Uv$. \qed
\end{proof}

\begin{observacao}[Importância Prática]
O Teorema de Slutsky é essencial para:
\begin{itemize}
    \item Substituir $\sigma$ por $S_n$ em estatísticas assintóticas
    \item Construir intervalos de confiança com parâmetros estimados
    \item Desenvolver testes de hipóteses práticos
\end{itemize}
\end{observacao}

\section{Teorema Central do Limite (Resultado 3.7.6.1(a))}

\begin{teorema}[TCL - Lindeberg-Lévy]
Sejam $X_1, \ldots, X_n$ v.a.'s i.i.d. com $\mu = \mathbb{E}\{X_i\} < \infty$ e $\sigma^2 = \mathrm{Var}\{X_i\} < \infty$. Então:
\[
Z_n = \sqrt{n} \left( \frac{\bar{X}_n - \mu}{\sigma} \right) = \frac{\sqrt{n}(\bar{X}_n - \mu)}{\sigma} \xrightarrow{d} N(0,1), \quad n \to \infty
\]
\end{teorema}

\begin{proof}[Esboço usando função geradora de momentos]
Defina $Y_i = \frac{X_i - \mu}{\sigma}$, então $Y_i$ são i.i.d. com $\mathbb{E}[Y_i] = 0$ e $\mathrm{Var}(Y_i) = 1$.

Note que:
\[
Z_n = \frac{\sqrt{n}(\bar{X}_n - \mu)}{\sigma} = \sqrt{n} \cdot \frac{1}{n}\sum_{i=1}^n \frac{X_i - \mu}{\sigma} = \frac{1}{\sqrt{n}}\sum_{i=1}^n Y_i
\]

A f.m.g. de $Z_n$ é:
\begin{align}
M_{Z_n}(t) &= \mathbb{E}\left[\exp\left(\frac{t}{\sqrt{n}}\sum_{i=1}^n Y_i\right)\right] \\
&= \prod_{i=1}^n \mathbb{E}\left[\exp\left(\frac{t}{\sqrt{n}} Y_i\right)\right] \\
&= \left[\mathbb{E}\left[e^{\frac{t}{\sqrt{n}} Y_1}\right]\right]^n \\
&= \left[M_{Y_1}\left(\frac{t}{\sqrt{n}}\right)\right]^n
\end{align}

Expandindo $M_{Y_1}(s)$ em série de Taylor em torno de $s = 0$:
\[
M_{Y_1}(s) = M_{Y_1}(0) + M'_{Y_1}(0) s + \frac{M''_{Y_1}(0)}{2} s^2 + o(s^2)
\]

Como $\mathbb{E}[Y_1] = 0$ e $\mathrm{Var}(Y_1) = 1$:
\begin{itemize}
    \item $M_{Y_1}(0) = 1$
    \item $M'_{Y_1}(0) = \mathbb{E}[Y_1] = 0$
    \item $M''_{Y_1}(0) = \mathbb{E}[Y_1^2] = 1$
\end{itemize}

Portanto:
\[
M_{Y_1}\left(\frac{t}{\sqrt{n}}\right) = 1 + 0 + \frac{1}{2} \cdot \frac{t^2}{n} + o\left(\frac{t^2}{n}\right) = 1 + \frac{t^2}{2n} + o\left(\frac{1}{n}\right)
\]

Logo:
\[
M_{Z_n}(t) = \left[1 + \frac{t^2}{2n} + o\left(\frac{1}{n}\right)\right]^n
\]

Usando o fato que $\left(1 + \frac{a}{n}\right)^n \to e^a$:
\[
M_{Z_n}(t) \xrightarrow{n \to \infty} e^{t^2/2} = M_Z(t)
\]
onde $Z \sim N(0,1)$.

Pelo teorema de continuidade de Lévy, $Z_n \xrightarrow{d} N(0,1)$. \qed
\end{proof}

\begin{observacao}[Complexidade da Demonstração]
Esta é uma das provas mais técnicas do curso, envolvendo:
\begin{enumerate}
    \item Manipulação de funções geradoras de momentos
    \item Expansão em série de Taylor
    \item Resultados limites clássicos
    \item Teorema de continuidade de Lévy
\end{enumerate}
\end{observacao}

\section{Teorema de Mann-Wald / Método Delta (3.7.6.2(a))}

\begin{teorema}[Teorema de Mann-Wald (Método Delta)]
Seja $\{T_n, n \geq 1\}$ uma sequência de v.a.'s reais tais que
\[
\sqrt{n} \, (T_n - \theta) \xrightarrow{n \to \infty} N\left(0, \sigma^2(\theta)\right)
\]
Seja $g(\cdot)$ uma função contínua de valor real com derivada $g'(\theta)$ finita e não nula. Então:
\[
\sqrt{n} \left[ g(T_n) - g(\theta) \right] \xrightarrow{n \to \infty} N\left(0, \sigma^2(\theta) \, [g'(\theta)]^2 \right)
\]
\end{teorema}

\begin{proof}
Considere:
\[
\sqrt{n} \left[ g(T_n) - g(\theta) \right] = U_n \cdot V_n
\]
onde
\[
U_n = \sqrt{n} (T_n - \theta) \quad \text{e} \quad V_n = \frac{g(T_n) - g(\theta)}{T_n - \theta}
\]

Por hipótese, $U_n \xrightarrow{d} N(0, \sigma^2(\theta))$.

Note que:
\[
T_n - \theta = \frac{U_n}{\sqrt{n}}
\]

Como $U_n \xrightarrow{d} N(0, \sigma^2)$ (limitado em distribuição) e $\frac{1}{\sqrt{n}} \xrightarrow{P} 0$, pelo Teorema de Slutsky:
\[
T_n - \theta = U_n \cdot \frac{1}{\sqrt{n}} \xrightarrow{P} 0
\]

Portanto, $T_n \xrightarrow{P} \theta$.

Agora, pela definição de derivada:
\[
g'(\theta) = \lim_{x \to \theta} \frac{g(x) - g(\theta)}{x - \theta}
\]

Como $T_n \xrightarrow{P} \theta$ e a função $h(x) = \frac{g(x) - g(\theta)}{x - \theta}$ tem limite $g'(\theta)$ quando $x \to \theta$ (por continuidade de $g'$ em $\theta$), temos:
\[
V_n = h(T_n) \xrightarrow{P} g'(\theta)
\]

Aplicando o Teorema de Slutsky com $U_n \xrightarrow{d} N(0, \sigma^2)$ e $V_n \xrightarrow{P} g'(\theta)$:
\[
\sqrt{n} \left[ g(T_n) - g(\theta) \right] = U_n \cdot V_n \xrightarrow{d} N(0, \sigma^2) \cdot g'(\theta) = N\left(0, [g'(\theta)]^2 \sigma^2\right)
\]
\qed
\end{proof}

\begin{observacao}[Estrutura da Prova]
A prova combina elegantemente:
\begin{enumerate}
    \item Aproximação de Taylor de primeira ordem (implicitamente via definição de derivada)
    \item Teorema de Slutsky (duas vezes!)
    \item Teorema da função contínua
\end{enumerate}
É uma demonstração que sintetiza várias ferramentas do curso.
\end{observacao}

\section{Teorema Central do Limite para Variância Amostral (3.7.6.3(a))}

\begin{teorema}[TCL para $S_n^2$]
Sejam $X_1, \ldots, X_n$ v.a.'s i.i.d. com média $\mu$, variância $\sigma^2$ e $\mu_4 = \mathbb{E}[(X_1 - \mu)^4]$. Assuma que $0 < \mu_4 < \infty$ e $\mu_4 > \sigma^4$ (curtose $> 1$). Então:
\[
\sqrt{n} \left( S_n^2 - \sigma^2 \right) \xrightarrow[n \to \infty]{d} N\left(0, \mu_4 - \sigma^4\right)
\]
\end{teorema}

\begin{proof}
Considere 
\[
W_n \triangleq (n-1) n^{-1} S_n^2, \quad Y_i \triangleq (X_i - \mu)^2 \quad \text{para } i = 1, \ldots, n
\]
e 
\[
\overline{Y}_n = n^{-1} \sum_{i=1}^n Y_i.
\]

Assim,
\begin{align}
W_n &= \frac{1}{n} \sum_{i=1}^n (X_i - \overline{X}_n)^2 \\
&= \frac{1}{n} \sum_{i=1}^n \left( X_i - \mu + \mu - \overline{X}_n \right)^2 \\
&= \frac{1}{n} \sum_{i=1}^n \left[ (X_i - \mu)^2 + 2(X_i - \mu)(\mu - \overline{X}_n) + (\mu - \overline{X}_n)^2 \right] \\
&= \frac{1}{n} \sum_{i=1}^n (X_i - \mu)^2 - 2(\overline{X}_n - \mu)^2 + (\overline{X}_n - \mu)^2 \\
&= \overline{Y}_n - (\overline{X}_n - \mu)^2
\end{align}

Assim, vale-se
\begin{align}
\sqrt{n} \left( W_n - \sigma^2 \right) &= \sqrt{n} \left( \overline{Y}_n - \sigma^2 \right) - \sqrt{n} \left( \overline{X}_n - \mu \right)^2 \\
&= U_n + V_n
\end{align}

Note que $\{ Y_i \} \stackrel{\text{i.i.d.}}{\sim} \left[ \mathbb{E}[Y_i] = \sigma^2, \ \mathrm{Var}(Y_i) = \mu_4 - \sigma^4 \right]$.

Daí, pelo Teorema Central do Limite:
\[
U_n \xrightarrow[n \to \infty]{d} N(0, \mu_4 - \sigma^4) \quad \text{como já discutido,}
\]
e
\[
U_n \xrightarrow[n \to \infty]{P} 0
\]

Daí, pelo Teorema de Slutsky:
\[
\sqrt{n} \left( W_n - \sigma^2 \right) \xrightarrow[n \to \infty]{d} N(0, \mu_4 - \sigma^4)
\]

Agora escrevemos
\begin{align}
\sqrt{n} \left( S_n^2 - \sigma^2 \right) &= \sqrt{n} \left( \frac{n}{n-1} W_n - \sigma^2 \right) \\
&= \sqrt{n} \left( \frac{n}{n+1} - 1 + 1 \right) \left( W_n - \sigma^2 \right) \\
&= \sqrt{n} \left( W_n - \sigma^2 \right) + \frac{\sqrt{n}}{n-1} W_n
\end{align}

Como 
\[
\sqrt{n} \left( W_n - \sigma^2 \right) \xrightarrow[n \to \infty]{d} N(0, \mu_4 - \sigma^4),
\]
\[
W_n \xrightarrow[n \to \infty]{P} \sigma^2, \quad \frac{\sqrt{n}}{n-1} \xrightarrow[n \to \infty]{} 0, \quad \text{e} \quad \frac{\sqrt{n}}{n-1} W_n \xrightarrow[n \to \infty]{P} 0,
\]

então, do Teorema de Slutsky:
\[
\sqrt{n} \left( S_n^2 - \sigma^2 \right) \xrightarrow[n \to \infty]{d} N(0, \mu_4 - \sigma^4)
\]
\qed
\end{proof}

\begin{observacao}[Importância Prática]
Este teorema permite construir intervalos de confiança e testes de hipóteses para $\sigma^2$ sem assumir normalidade da população, apenas usando propriedades assintóticas.
\end{observacao}

\section{Teorema da Função Contínua para Convergência em Distribuição (3.7.6.4(a))}

\begin{teorema}[Teorema da Função Contínua para Convergência em Distribuição]
Sejam $\{U_n, n \geq 1\}$ uma sequência de v.a.'s reais e $U$ uma variável real. Seja $g(\cdot)$ uma função contínua de valor real. Se $U_n \xrightarrow[n \to \infty]{d} U$, então:
\[
g(U_n) \xrightarrow[n \to \infty]{d} g(U)
\]
\end{teorema}

\begin{observacao}
Este é o análogo para convergência em distribuição do Resultado 5P (para convergência em probabilidade). É essencial para transformações de estatísticas que convergem em distribuição.
\end{observacao}

\subsection*{Exercício (11) Q: Aplicação do Teorema}

\textbf{Enunciado:} Sejam $X_1, \ldots, X_n$ v.a.'s i.i.d. reais tais que $\mu = \mathbb{E}\{X_i\} < \infty$ e $\sigma^2 = \mathrm{Var}\{X_i\} < \infty$. Mostre que
\[
n \left( \frac{\overline{X}_n - \mu}{\sigma} \right)^2 \xrightarrow[n \to \infty]{d} Q,
\]
tal que $Q \sim \chi^2_1$.

\textbf{Solução:} Defina $Z_n \triangleq \sqrt{n} \left( \frac{\overline{X}_n - \mu}{\sigma} \right)$. Pelo TCL, tem-se
\[
Z_n \xrightarrow[n \to \infty]{d} Z \sim N(0,1).
\]

Pelo Teorema (3.7.6.4(a)), como $g(x) = x^2$ é uma função contínua,
\[
g(Z_n) = n \left( \frac{\overline{X}_n - \mu}{\sigma} \right)^2 \xrightarrow[n \to \infty]{d} Z^2,
\]
isto é, converge para $Q \sim \chi^2_1$ (pois o quadrado de uma $N(0,1)$ tem distribuição qui-quadrado com 1 grau de liberdade). \qed

\section{Estimadores Consistentes (Seção 3.7)}

\subsection{Definição de Consistência}

\begin{definicao}[Consistência no Sentido Fraco (3.7.1)]
Seja 
\[
\{T_n = T_n(X_1, \ldots, X_n); n \geq 1\}
\]
uma sequência de estimadores para $\tau(\theta)$ tal que $\theta \in \Theta \subset \mathbb{R}^p$.

$T_n$ é \textbf{consistente no sentido fraco} para $\tau(\theta)$ se, e só se
\[
T_n \xrightarrow[n \to \infty]{P} \tau(\theta)
\]

$T_n$ é \textbf{inconsistente} para $\tau(\theta)$ se $T_n$ não converge em probabilidade para $\tau(\theta)$.
\end{definicao}

\begin{observacao}[Observação 1]
Dados $\varepsilon > 0$ e $\delta \in (0,1)$, existe $n_0 = n_0(\varepsilon, \delta, \theta)$ tal que:
\[
P_\theta\{|T_n - \theta| > \varepsilon\} \leq \delta \ \Longleftrightarrow \ P_\theta\{|T_n - \theta| \leq \varepsilon\} \geq 1 - \delta, \quad \forall n \geq n_0
\]
\end{observacao}

\begin{observacao}[Observação 2]
$T_n$ é consistente se, e só se
\[
\lim_{n \to \infty} P_\theta\{|T_n - \theta| > \varepsilon\} = 0
\]
ou equivalentemente
\[
\lim_{n \to \infty} P_\theta\{|T_n - \theta| \leq \varepsilon\} = 1
\]
\end{observacao}

\begin{observacao}[Consistência via EQM]
$T_n \xrightarrow[n \to \infty]{P} \theta$ se $EQM_\theta[T_n] \xrightarrow[n \to \infty]{} 0$.

Isto pode ser verificado pela desigualdade de Chebyshev. Para qualquer $\varepsilon > 0$ e $\theta \in \Theta$:
\[
P_\theta\left( |T_n - \theta| > \varepsilon \right) \leq \frac{E_\theta\left[ (T_n - \theta)^2 \right]}{\varepsilon^2} = \frac{EQM_\theta[T_n]}{\varepsilon^2} \xrightarrow[n \to \infty]{} 0
\]

Deste último resultado, $T_n \xrightarrow[n \to \infty]{P} \theta$ implica que se $T_n$ é centrado, basta checar $Var_\theta[T_n] \xrightarrow[n \to \infty]{} 0$.
\end{observacao}

\subsection{Exemplo: Consistência do Máximo da Uniforme}

\textbf{Questão (3.23):} Sejam $X_1, \ldots, X_n$ uma amostra de $X \sim U(0, \theta)$. Mostre que o estimador de MV para $\theta$, $T_n = X_{n:n}$, é consistente para $\theta$.

\textbf{Solução:} A função de distribuição de $T_n = X_{n:n}$ é:
\[
F_{T_n}(t) = P_\theta(T_n \leq t) = P_\theta \left\{ \bigcap_{i=1}^n X_i \leq t \right\} = \left[ F_{X_1}(t) \right]^n = 
\begin{cases}
0, & t < 0, \\
\left( \frac{t}{\theta} \right)^n, & 0 \leq t \leq \theta, \\
1, & t > \theta
\end{cases}
\]

Para $\varepsilon > 0$:
\begin{align}
P_\theta \left\{ |X_{n:n} - \theta| < \varepsilon \right\} &= P_\theta \left\{ \theta - \varepsilon < X_{n:n} < \theta + \varepsilon \right\} \\
&= P_\theta \left\{ \theta - \varepsilon < X_{n:n} < \theta \right\} \\
&= F_{X_{n:n}}(\theta) - F_{X_{n:n}}(\theta - \varepsilon) \\
&= 
\begin{cases}
1, & \varepsilon \geq \theta, \\
1 - \left( \frac{\theta - \varepsilon}{\theta} \right)^n, & \varepsilon < \theta
\end{cases}
\end{align}

Portanto:
\[
\lim_{n \to \infty} P_\theta \left\{ |X_{n:n} - \theta| < \varepsilon \right\} = 1
\]

Logo, $X_{n:n} \xrightarrow[n \to \infty]{P} \theta$.

\begin{observacao}[Tamanho Amostral Mínimo]
Um fato interessante é que se pode obter o tamanho amostral mínimo $n_0$ tal que
\[
P\left( \left| X_{n:n} - \theta \right| < \varepsilon \right) \geq 1 - \delta,
\]
em que $\varepsilon > 0$ e $\delta \in (0,1)$ são constantes pré-especificadas para $\varepsilon < \theta$.

De $1 - \left( \frac{\theta - \varepsilon}{\theta} \right)^n \geq 1 - \delta$, obtemos $\left( \frac{\theta - \varepsilon}{\theta} \right)^n \leq \delta$, logo:
\[
n \geq \frac{\log \delta}{\log \left( \frac{\theta - \varepsilon}{\theta} \right)}
\]

Assim:
\[
n_0 = \left\lceil \frac{\log \delta}{\log \left( \frac{\theta - \varepsilon}{\theta} \right)} \right\rceil + 1
\]

Para $\theta \leq \varepsilon$: $n_0 = 1$.
\end{observacao}

\section{Propriedades Assintóticas dos EMVs (Seção 3.8)}

\subsection{Eficiência Relativa Assintótica}

\begin{definicao}[Eficiência Relativa Assintótica (3.4.1)]
Se dois estimadores $T_n^{(1)}$ e $T_n^{(2)}$ para $g(\theta)$ são ambos assintoticamente normais:
\[
\sqrt{n} \left( T_n^{(1)} - g(\theta) \right) \xrightarrow[n \to \infty]{d} N\left(0, \sigma_1^2(\theta)\right)
\]
e
\[
\sqrt{n} \left[ T_n^{(2)} - g(\theta) \right] \xrightarrow[n \to \infty]{d} N\left(0, \sigma_2^2(\theta)\right),
\]
então a \textbf{eficiência relativa assintótica} de $T^{(2)}$ com respeito a $T^{(1)}$ é definida como
\[
\frac{\sigma_1^2(\theta)}{\sigma_2^2(\theta)}
\]
\end{definicao}

\begin{observacao}
Os EMVs são assintoticamente eficientes, ou seja, atingem a menor variância assintótica possível (o limite inferior de Cramér-Rao assintótico).
\end{observacao}

\subsection{Teorema Central do Limite para EMVs}

\begin{teorema}[TCL para os EMVs (3.8.1)]
Sejam $X_1, \ldots, X_n$ uma amostra de $X$ com fdp (ou fmp) $f(x; \theta)$ para $x \in \mathbb{X} \subset \mathbb{R}$ e $\theta \in \Theta \subset \mathbb{R}$ tal que $\Theta$ é um intervalo aberto.

Assuma que:

\textbf{(A1)} $\theta \mapsto f(x; \theta)$ é três vezes diferenciável sobre $\Theta$, $\forall x \in \mathbb{X}$.

\textbf{(A2)} Condições de regularidade para troca de derivação e integração:
\[
\int_{\mathbb{X}} \frac{\partial}{\partial \theta} f(x; \theta) \, dx = 0 \quad \text{e} \quad \int_{\mathbb{X}} \frac{\partial^2}{\partial \theta^2} f(x; \theta) \, dx = 0
\]

\textbf{(A3)} A informação de Fisher é finita e positiva:
\[
0 < I_X(\theta) \triangleq \mathbb{E}_\theta \left[ \left( \frac{\partial \log f(x; \theta)}{\partial \theta} \right)^2 \right] < \infty, \quad \forall \theta \in \Theta
\]

\textbf{(A4)} Para cada $\theta_0 \in \Theta$, existe $\varepsilon = \varepsilon(\theta_0) > 0$ tal que
\[
\left| \frac{\partial^3 \log f(x; \theta)}{\partial \theta^3} \right| \leq g(x), \quad \forall \theta \in [\theta_0 - \varepsilon, \theta_0 + \varepsilon],
\]
em que $\int_X g(x) f(x; \theta) \, dx < \infty$.

\textbf{(A5)} A equação de verossimilhança
\[
\frac{\partial l(\theta)}{\partial \theta} = 0 \quad \Leftrightarrow \quad \sum_{i=1}^n \frac{\partial \log f(x_i; \theta)}{\partial \theta} = 0
\]
tem uma solução consistente $\hat{\theta}_n$.

Então:
\[
\sqrt{n} (\hat{\theta}_n - \theta_0) \xrightarrow[n \to \infty]{d} N\left(0, I_X^{-1}(\theta_0)\right).
\]
\end{teorema}

\begin{observacao}[Interpretação]
Este teorema fundamental estabelece que, sob condições de regularidade:
\begin{enumerate}
    \item Os EMVs são assintoticamente não-viesados
    \item Os EMVs são assintoticamente normais
    \item Os EMVs atingem a variância assintótica mínima (limite de Cramér-Rao)
    \item A taxa de convergência é $\sqrt{n}$
\end{enumerate}
Estas propriedades justificam a popularidade do método de máxima verossimilhança na prática estatística.
\end{observacao}

\newpage
\section{Ranking de Prioridade das Demonstrações}

Esta seção apresenta um ranking das demonstrações mais importantes para estudo, considerando três critérios com pesos diferentes:
\begin{itemize}
    \item \textbf{Complexidade Técnica} (30\%): Dificuldade matemática e número de passos
    \item \textbf{Importância Fundamental} (40\%): Base para outros resultados e centralidade no curso
    \item \textbf{Aplicabilidade em Questões} (30\%): Frequência de uso em exercícios e exames
\end{itemize}

\subsection{Tabela de Avaliação}

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Teorema} & \textbf{Compl.} & \textbf{Import.} & \textbf{Aplic.} & \textbf{Nota Final} \\
 & (0-10) & (0-10) & (0-10) & (ponderada) \\
\hline
TCL & 9.5 & 10 & 10 & \textbf{9.85} \\
\hline
Método Delta (Mann-Wald) & 8.5 & 9 & 9.5 & \textbf{8.95} \\
\hline
Teorema de Slutsky & 7.0 & 9.5 & 9.0 & \textbf{8.65} \\
\hline
TCL para $S_n^2$ & 8.0 & 8.0 & 8.5 & \textbf{8.15} \\
\hline
LFGN de Khinchin & 8.0 & 8.5 & 7.5 & \textbf{8.05} \\
\hline
Teorema da Função Contínua (Dist.) & 5.5 & 7.5 & 8.0 & \textbf{7.15} \\
\hline
Teorema da Função Contínua (Prob.) & 6.0 & 8.0 & 8.0 & \textbf{7.40} \\
\hline
LFGN Versão Simples & 5.0 & 7.5 & 7.0 & \textbf{6.65} \\
\hline
Convergência via Momentos & 4.5 & 7.0 & 6.5 & \textbf{6.10} \\
\hline
\end{tabular}
\caption{Avaliação e ranking dos teoremas (atualizado)}
\end{table}

\subsection{Ranking Final: Top 5 Demonstrações}

\subsubsection{1º Lugar: Teorema Central do Limite (Nota: 9.85)}

\textbf{Por que estudar em detalhes:}
\begin{itemize}
    \item É o teorema mais importante da estatística
    \item Demonstração mais complexa e técnica do curso
    \item Utiliza múltiplas ferramentas: fmg, série de Taylor, resultados limite
    \item Praticamente garantido ser cobrado em avaliações
    \item Base para toda inferência estatística assintótica
\end{itemize}

\textbf{Dica de estudo:} Entenda cada passo da expansão em série de Taylor e como os resultados limite (R.2) e (R.3) são aplicados.

\subsubsection{2º Lugar: Método Delta / Teorema de Mann-Wald (Nota: 8.95)}

\textbf{Por que estudar em detalhes:}
\begin{itemize}
    \item Síntese elegante de várias técnicas (Slutsky, função contínua)
    \item Extremamente prático para questões de transformações
    \item Demonstração que mostra maturidade matemática
    \item Frequentemente aparece em questões aplicadas
\end{itemize}

\textbf{Dica de estudo:} Foque na estrutura da prova: decomposição em $U_n \cdot V_n$, depois aplicação dupla de Slutsky.

\subsubsection{3º Lugar: Teorema de Slutsky (Nota: 8.65)}

\textbf{Por que estudar em detalhes:}
\begin{itemize}
    \item Ferramenta essencial para questões práticas
    \item Permite trabalhar com parâmetros desconhecidos
    \item Prova relativamente acessível mas profunda
    \item Usado na prova do Método Delta
\end{itemize}

\textbf{Dica de estudo:} Entenda o conceito de "limitado em probabilidade" e como ele é usado na prova.

\subsubsection{4º Lugar: TCL para Variância Amostral $S_n^2$ (Nota: 8.15)}

\textbf{Por que estudar em detalhes:}
\begin{itemize}
    \item Aplicação prática importante do TCL
    \item Combina múltiplas técnicas: TCL original + Slutsky
    \item Essencial para inferência sobre variâncias
    \item Demonstra manipulação algébrica sofisticada
    \item Muito útil para questões aplicadas
\end{itemize}

\textbf{Dica de estudo:} Foque na decomposição $W_n = \overline{Y}_n - (\overline{X}_n - \mu)^2$ e como ela leva à aplicação do Teorema de Slutsky.

\subsubsection{5º Lugar: LFGN de Khinchin (Nota: 8.05)}

\textbf{Por que estudar em detalhes:}
\begin{itemize}
    \item Versão mais geral que a LFGN simples
    \item Usa técnica de fmg similar ao TCL (boa preparação)
    \item Demonstração de complexidade média
    \item Fundamento para consistência de estimadores
\end{itemize}

\textbf{Dica de estudo:} Compare com a prova do TCL para ver as similaridades e diferenças na técnica.

\subsection{Estratégia de Estudo Recomendada}

\begin{enumerate}
    \item \textbf{Primeira semana:} Estude profundamente o TCL (1º lugar). Refaça a prova múltiplas vezes até dominar.
    
    \item \textbf{Segunda semana:} Método Delta e Slutsky (2º e 3º lugares). Veja como trabalham juntos.
    
    \item \textbf{Terceira semana:} TCL para $S_n^2$ (4º lugar) - observe como combina TCL original e Slutsky. Depois LFGN de Khinchin (5º lugar).
    
    \item \textbf{Quarta semana:} Estude os novos tópicos: Consistência de estimadores (definições e exemplos como $X_{n:n}$ para uniforme) e propriedades assintóticas dos EMVs.
    
    \item \textbf{Revisão:} Compare as técnicas comuns entre as provas (Slutsky aparece em várias, incluindo TCL para $S_n^2$).
    
    \item \textbf{Prática:} Resolva exercícios aplicando cada teorema para fixar quando usar cada um. Pratique especialmente transformações usando Método Delta e Teorema da Função Contínua.
\end{enumerate}

\subsection{Observações Finais}

\begin{itemize}
    \item As provas dos resultados 1P e 2P (LFGN simples e convergência via momentos) são mais diretas e servem como "aquecimento"
    
    \item O TCL é o "ápice técnico" - domine-o e as outras provas parecerão mais acessíveis
    
    \item O TCL para $S_n^2$ é uma excelente aplicação que combina múltiplas técnicas estudadas
    
    \item Entender \emph{por que} cada teorema é verdadeiro é tão importante quanto saber os passos da prova
    
    \item Em avaliações, provas de TCL e Método Delta geralmente valem mais pontos
    
    \item Os novos tópicos de consistência e EMVs são essenciais para a compreensão de inferência assintótica
    
    \item O Teorema 3.8.1 (TCL para EMVs) é fundamental mas sua prova completa é muito técnica - foque em entender as condições (A1)-(A5) e suas interpretações
\end{itemize}

\end{document}

